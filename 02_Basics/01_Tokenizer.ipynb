{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46fcd37d",
   "metadata": {},
   "source": [
    "# 3. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2600fa29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\RAMU\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Resource punkt not found. - Please use the NLTK Downloader to obtain the resource:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4985432d",
   "metadata": {},
   "source": [
    "## 3.1. Sentence token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed9a52cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi, this is Rama.', 'I am a student.', 'I like NLP']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "text = \"Hi, this is Rama. I am a student. I like NLP\"\n",
    "\n",
    "sent_tokens = nltk.sent_tokenize(text)\n",
    "print(sent_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a979685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi, this is Rama.', 'I am a student.', 'I like NLP']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the English language model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Process the text with spaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "# Extract sentences from the processed document\n",
    "sentences = [sent.text for sent in doc.sents]\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7fe91a",
   "metadata": {},
   "source": [
    "## 3.2. Tokens of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c59cb58e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Hi', ',', 'this', 'is', 'Rama', '.'], ['I', 'am', 'a', 'student', '.'], ['I', 'like', 'NLP']]\n"
     ]
    }
   ],
   "source": [
    "tokens_of_sents = [list(nltk.word_tokenize(sent)) for sent in sent_tokens]\n",
    "print(tokens_of_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ba935d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[Hi, ,, this, is, Rama, .], [I, am, a, student, .], [I, like, NLP]]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the English language model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Process the text with spaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "# Extract tokens of sentences from the processed document\n",
    "tokens_of_sents = [[token for token in sent] for sent in doc.sents]\n",
    "print(tokens_of_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8944de82",
   "metadata": {},
   "source": [
    "## 3.3. Tokens of whole text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55996b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi', ',', 'this', 'is', 'Rama', '.', 'I', 'am', 'a', 'student', '.', 'I', 'like', 'NLP']\n"
     ]
    }
   ],
   "source": [
    "tokens_of_text = nltk.word_tokenize(text)\n",
    "print(tokens_of_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0337b0eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hi, ,, this, is, Rama, ., I, am, a, student, ., I, like, NLP]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the English language model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Process the text with spaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "# Extract tokens of text from the processed document\n",
    "tokens_of_text = [token for token in doc]\n",
    "print(tokens_of_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9e156d",
   "metadata": {},
   "source": [
    "## 3.4 Exersize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71ad97ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required libriry\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb6fd3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "nl = '''Unbidden, a memory came to me: Mal and I, sitting side by side in a chair in\n",
    "the Duke's library, flipping through the pages of a large leather-bound book.\n",
    "We’d happened on an illustration of a volcra: long, filthy claws; leathery wings;\n",
    "and rows of razor-sharp teeth for feasting on human flesh. They were blind from\n",
    "generations spent living and hunting in the Fold, but legend had it they could\n",
    "smell human blood from miles away. I’d pointed to the page and asked, “What is\n",
    "it holding?'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "113c5b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Unbidden', ',', 'a', 'memory', 'came', 'to', 'me', ':', 'Mal', 'and', 'I', ',', 'sitting', 'side', 'by', 'side', 'in', 'a', 'chair', 'in', 'the', 'Duke', \"'s\", 'library', ',', 'flipping', 'through', 'the', 'pages', 'of', 'a', 'large', 'leather-bound', 'book', '.', 'We', '’', 'd', 'happened', 'on', 'an', 'illustration', 'of', 'a', 'volcra', ':', 'long', ',', 'filthy', 'claws', ';', 'leathery', 'wings', ';', 'and', 'rows', 'of', 'razor-sharp', 'teeth', 'for', 'feasting', 'on', 'human', 'flesh', '.', 'They', 'were', 'blind', 'from', 'generations', 'spent', 'living', 'and', 'hunting', 'in', 'the', 'Fold', ',', 'but', 'legend', 'had', 'it', 'they', 'could', 'smell', 'human', 'blood', 'from', 'miles', 'away', '.', 'I', '’', 'd', 'pointed', 'to', 'the', 'page', 'and', 'asked', ',', '“', 'What', 'is', 'it', 'holding', '?']\n"
     ]
    }
   ],
   "source": [
    "# Now tokenize thw above nl\n",
    "word_tokens = word_tokenize(nl)\n",
    "\n",
    "# Printing the word tokens\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a80f3a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 107\n"
     ]
    }
   ],
   "source": [
    "# Check type and size of word_tokens\n",
    "print(type(word_tokens), len(word_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93df408f",
   "metadata": {},
   "source": [
    "### 3.4.1. Frequency of Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4bc38793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Unbidden': 1, ',': 6, 'a': 4, 'memory': 1, 'came': 1, 'to': 2, 'me': 1, ':': 2, 'Mal': 1, 'and': 4, 'I': 2, 'sitting': 1, 'side': 2, 'by': 1, 'in': 3, 'chair': 1, 'the': 4, 'Duke': 1, \"'s\": 1, 'library': 1, 'flipping': 1, 'through': 1, 'pages': 1, 'of': 3, 'large': 1, 'leather-bound': 1, 'book': 1, '.': 3, 'We': 1, '’': 2, 'd': 2, 'happened': 1, 'on': 2, 'an': 1, 'illustration': 1, 'volcra': 1, 'long': 1, 'filthy': 1, 'claws': 1, ';': 2, 'leathery': 1, 'wings': 1, 'rows': 1, 'razor-sharp': 1, 'teeth': 1, 'for': 1, 'feasting': 1, 'human': 2, 'flesh': 1, 'They': 1, 'were': 1, 'blind': 1, 'from': 2, 'generations': 1, 'spent': 1, 'living': 1, 'hunting': 1, 'Fold': 1, 'but': 1, 'legend': 1, 'had': 1, 'it': 2, 'they': 1, 'could': 1, 'smell': 1, 'blood': 1, 'miles': 1, 'away': 1, 'pointed': 1, 'page': 1, 'asked': 1, '“': 1, 'What': 1, 'is': 1, 'holding': 1, '?': 1}\n"
     ]
    }
   ],
   "source": [
    "# Required libriry\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "fdist = FreqDist()\n",
    "\n",
    "# for loop the each token\n",
    "for token in word_tokens:\n",
    "    fdist[token] = fdist[token] + 1\n",
    "    \n",
    "print(dict(fdist))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a61d57",
   "metadata": {},
   "source": [
    "### 3.4.2. Most frquents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31673d59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(',', 6), ('a', 4), ('and', 4), ('the', 4), ('in', 3), ('of', 3), ('.', 3), ('to', 2), (':', 2), ('I', 2), ('side', 2), ('’', 2), ('d', 2), ('on', 2), (';', 2), ('human', 2), ('from', 2), ('it', 2), ('Unbidden', 1), ('memory', 1)]\n"
     ]
    }
   ],
   "source": [
    "top_10 = fdist.most_common(20)\n",
    "print(top_10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
